[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "What’s Special About This Book?\nHonestly, nothing. It’s just another book on transformers—and there are many excellent ones out there.\nSo why write another one? I’ve noticed that much of the confusion learners have about the core of transformers—the “attention” mechanism—traces back to the name itself.\nMost courses treat “attention” as more than just a name—they build analogies on top of it (e.g., a word “attends to” other words) and explain transformer models without a consistent level of abstraction (e.g., the subject of “attend” alternates between words and models; the “self” in self-attention refers to the input rather than a word. etc.). If you’re already feeling confused, this is why.\nIn this book, I try to keep analogies minimal. I treat “attention” as just a name—nothing more—and explain things without complex analogies that complicate rather than clarify. Hopefully, this approach will work for some learners.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#what-is-this-book-about",
    "href": "overview.html#what-is-this-book-about",
    "title": "Overview",
    "section": "What Is This Book About?",
    "text": "What Is This Book About?\nThis book is still a work in progress. Here’s what I have planned:\nPart 1: The One Mechanism\nHow text becomes numbers (Chapter 1), how tokens exchange context through relevance-weighted averaging (Chapter 2), and the full mathematical walkthrough of attention (Chapter 3).\nPart 2: The Twofold Training\nHow these models acquire their capabilities. Pre-training on vast text corpora (Chapter 4) and fine-tuning for specific tasks (Chapter 5).\nPart 3: The Three Wizards\nThe major architectural families. Encoders (Chapter 6), decoders (Chapter 7), and everything in between (Chapter 8).\nPart 4: The Appendices\nPractical matters. Computational considerations (Chapter 9), evaluation methods (Chapter 10), and modern methods (Chapter 11).",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#prerequisites",
    "href": "overview.html#prerequisites",
    "title": "Overview",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou’ll benefit from having a basic grasp of ML concepts. Familiarity with neural networks, understanding that “training” is about adjusting parameters for improvement, and knowing that matrices are just collections of numbers will help. If you come across terms that puzzle you, like “softmax” or “gradient”, don’t hesitate to look them up.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "module-01.html",
    "href": "module-01.html",
    "title": "1  Text, Tokens, and Numbers",
    "section": "",
    "text": "1.1 Machines Need Numbers\nBefore a Transformer can answer questions or write novels, text must be turned into numbers. To a computer, text is not meaning — it is just a sequence of bytes.\nConsider the word “horse.”\nThe letters h-o-r-s-e do not inherently imply a hoofed mammal that runs fast. In German, the same animal is “Pferd.” In Chinese, it is “馬.” The spellings are completely different, yet they refer to the same real-world creature — they are just symbols that represent the same concept.\nArtificial neural networks, the core components of AI models, operate on numbers rather than not symbols. So before a model can process language, we must convert text into numbers.\nWhat We Need\nWe need a systematic way to convert text into numerical vectors such that:\nThis conversion process has three stages.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Text, Tokens, and Numbers</span>"
    ]
  },
  {
    "objectID": "module-01.html#machines-need-numbers",
    "href": "module-01.html#machines-need-numbers",
    "title": "1  Text, Tokens, and Numbers",
    "section": "",
    "text": "Every piece of text maps to a specific numerical representation.\nThe format is consistent (the model always receives the same input shape).",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Text, Tokens, and Numbers</span>"
    ]
  },
  {
    "objectID": "module-01.html#text-tokens-vectors",
    "href": "module-01.html#text-tokens-vectors",
    "title": "1  Text, Tokens, and Numbers",
    "section": "1.2 Text → Tokens → Vectors",
    "text": "1.2 Text → Tokens → Vectors\nConsider the input: \"the ring fell\"\n\n1.2.1 Stage 1: The Breaking of the Text\nFirst, we must break the text into smaller units — tokens (For simplicity, think of it as individual words). Each token maps to a unique integer ID via a static lookup table (the vocabulary).\nThis process is called tokenization.\n\n\n\n\n\n---\nconfig:\n  themeVariables:\n    fontFamily: \"monospace\"\n  flowchart:\n    nodeSpacing: 8\n    rankSpacing: 25\n    padding: 10\n---\nflowchart BT\n    Input(\"Raw Text&lt;br&gt;'the ring fell'\")\n    subgraph Tokenize[\" Tokenization\"]\n        direction BT\n        T1(\"the\")\n        T2(\"ring\")\n        T3(\"fell\")\n        ID1(\"101\")\n        ID2(\"205\")\n        ID3(\"310\")\n    end\n    Output(\"Token IDs&lt;br&gt;[101, 205, 310]\")\n    \n    Input --&gt; Tokenize --&gt; Output\n    T1 -.-&gt; ID1\n    T2 -.-&gt; ID2\n    T3 -.-&gt; ID3    \n\n    %% Styles\n    style Input fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style Tokenize fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style T1 fill:#fff,stroke:#4a3b2c\n    style T2 fill:#fff,stroke:#4a3b2c\n    style T3 fill:#fff,stroke:#4a3b2c\n    style ID1 fill:#fff,stroke:#4a3b2c\n    style ID2 fill:#fff,stroke:#4a3b2c\n    style ID3 fill:#fff,stroke:#4a3b2c\n    style Output fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat is the “Vocabulary”?\n\n\n\n\n\nIn everyday language, vocabulary refers to the words a person knows. In NLP, the Vocabulary (\\(V\\)) is the fixed, finite list of all unique tokens the model knows.\nThink of it as a dictionary where every entry is assigned a specific number (ID).\n\nSize: Typical vocabularies contain 30,000 to 100,000 unique tokens.\nFixed: Once training starts, the vocabulary cannot change.\n\n\n\n\n\n\n\n\n\n\nNoteConcerning Tokenization\n\n\n\n\n\nWe can tokenize text in three ways:\n\nCharacter-level: Split “the” into ['T', 'h', 'e'].\n\nIssue: Sequences become too long, and the model must learn spelling from scratch.\n\nWord-level: Split “the ring fell” into ['the', 'ring', 'fell'].\n\nIssue: The vocabulary explodes (hundreds of thousands of words), and unknown words like “riing” cause errors.\n\nSubword-level (The Modern Standard):\n\nCommon words remain whole (“ring” → ['ring']).\nRare words are split into meaningful chunks (“transformer” → ['trans', 'former']).\n\n\nModern models use Subword Tokenization, which strikes a balance between vocabulary size and meaning.\nMost models use Byte-Pair Encoding (BPE). It starts with individual characters and iteratively merges the most frequent adjacent pairs until a target vocabulary size is reached (e.g., 50,000). This strikes the perfect balance between efficiency and meaning.\n\n\n\n\n\n\n\n\n\nTipNote: Special Tokens\n\n\n\n\n\nBeyond words, models use special tokens to mark structure or unknown words:\n\n[CLS]: Classification summary (BERT).\n[SEP]: Separator between sentences.\n[MASK]: Placeholder for prediction.\n[PAD]: Padding for batching.\n[UNK]: Unknown word.\n\n\n\n\n\n\n1.2.2 Stage 2: Embedding\nHaving numerical tokens (IDs) is not enough. An ID simply indexes a token in a vocabulary, but doesn’t carry information itself. Besides, we cannot feed token IDs (e.g., [101, 205, 310]) directly into a neural network because they imply false mathematical relationships (e.g., 310 &gt; 101 suggests fell is somehow “greater than” the).\nInstead, we convert each ID into a vector of numbers using an embedding matrix.\n\nWhat Is the Embedding Matrix?\nThink of the embedding matrix as a dictionary, but instead of definitions, each entry (i.e. each row) contains a list of numbers (an embedding vector), representing a token in the vocabulary:\n\\[\nW_E \\in \\mathbb{R}^{|V| \\times d_{model}}\n\\]\n\n\\(|V|\\) = vocabulary size (number of entries in the dictionary)\n\\(d_{model}\\) = numbers of dimensions used to describe each token\nWe’ll use \\(d_{model} = 4\\) throughout the book to keep examples consistent.\n\nFor our sequence “the ring fell” (3 tokens), we look up 3 rows from \\(W_E\\):\n\\[\nE = \\begin{bmatrix}\n0.1 & 0.2 & 0.3 & 0.4 \\\\\n0.5 & 0.6 & 0.7 & 0.8 \\\\\n0.2 & 0.4 & 0.6 & 0.8\n\\end{bmatrix}\n\\begin{matrix}\n\\leftarrow \\text{``the\" (ID: 101)} \\\\\n\\leftarrow \\text{``ring\" (ID: 205)} \\\\\n\\leftarrow \\text{``fell\" (ID: 310)}\n\\end{matrix}\n\\]\nIntuitively, each row represents a token’s unique identity—a numerical fingerprint that says what it is, but not yet what it means here.\n\n\nWhat Do These Numbers Represent?\nEach number in a token’s embedding vector describes the token along one dimension (a.k.a. feature). You can think of it like this: just as a person might be described (and thus identified) along multiple dimensions like height, age, hair color, a token is described along some learned dimensions.\n\nNote that an embedding only helps us identify a token, but not understand it. For example, the embedding for ring might encode properties like:\ncommon English word, \ncan function as noun or verb, \nhas certain phonetic properties, \nfrequently co-occurs with words like X, Y, Z...\nBut this vector doesn’t tell us whether “ring” means jewelry or sound in any particular sentence: Meaning requires context, which we don’t have yet.\n\n\n\n\n\n\nNoteAre Embedding Features Human-Interpretable?\n\n\n\n\n\nNot really. Unlike a dictionary definition you can read, these features are learned patterns that emerge from training on massive amounts of text. Dimension 47 might correlate with “noun-ness” and dimension 203 with “abstract vs. concrete,” but we can’t easily inspect them.\nWhat we can say: tokens with similar identities end up with similar vectors. “King” and “queen” will be closer together than “king” and “banana”—not because someone programmed royalty features, but because they appear in similar contexts during training.\n\n\n\n\n\n\n\n\n\nNoteHistorical Note: Word2Vec and Static Embeddings\n\n\n\n\n\nThe idea of representing words as vectors predates Transformers. In 2013, Word2Vec demonstrated that you could learn embeddings where arithmetic worked: vector(“king”) - vector(“man”) + vector(“woman”) ≈ vector(“queen”).\nThese were revolutionary but static—each word had exactly one vector, regardless of context. The word “ring” would get a single embedding averaging across all its uses (jewelry, sound, boxing ring, tree ring…).\nTransformers start with similar static embeddings but then update them based on context—which is exactly what the next chapter will introduce.\n\n\n\n\n\n\n1.2.3 Stage 3: Positional Encoding\nIf we input “fell ring the” instead of “the ring fell,” the embedding matrix simply swaps rows. Because the Transformer processes all tokens simultaneously (permutation invariance), it cannot inherently distinguish “dog bites man” from “man bites dog.”\nThe Solution: We inject position information by adding a position-specific vector (\\(P\\)) to each token embedding (\\(E\\)).\n\\[X = E + P\\]\nNow, each embedding in the ouput matrix, \\(X\\), captures what properties the token carries and where it is located. In the next chapter, we will see how transformers use this matrix to understand the text.\n\n\n\n\n\n\nTipTechnical Detail: The Math of Position\n\n\n\n\n\nOriginal Transformers used fixed sinusoidal functions:\n\\[PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\]\nThis allows the model to learn relative positions (e.g., “attend to the word 3 steps back”). Modern models often use learned positional embeddings or Rotary Positional Embeddings (RoPE).\nWhy Add and not Concatenate? Adding preserves dimensionality (\\(d_{model}\\)) for efficiency. In high-dimensional space, semantic and positional information are easily separable by the model.\n\n\n\n\n\n1.2.4 The Journey Thus Far\nWe have converted text into a numerical matrix \\(X\\) of shape \\((N \\times d_{model})\\).\n\n\n\n\n\n---\nconfig:\n  themeVariables:\n    fontFamily: \"monospace\"\n  flowchart:\n    nodeSpacing: 35\n    rankSpacing: 25\n    padding: 10\n---\nflowchart BT\n    A(\"Raw Text&lt;br&gt;'the ring fell'\")\n    B[[\"Tokenization\"]]\n    C(\"Tokens (IDs)&lt;br&gt;\")\n    D[[\"Embedding Lookup\"]]\n    E(\"Token Embeddings (E)&lt;br&gt;(3 × d&lt;sub&gt;model&lt;/sub&gt;)\")\n    F(\"Position IDs&lt;br&gt;\")\n    G[[\"Positional Encoding\"]]\n    H(\"Position Vectors (P)&lt;br&gt;(3 × d&lt;sub&gt;model&lt;/sub&gt;)\")\n    I((\"\\+\"))\n    J(\"Matrix X&lt;br&gt;(3 × d&lt;sub&gt;model&lt;/sub&gt;)\")\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; I\n    F --&gt; G --&gt; H --&gt; I\n    I --&gt; J\n    C -.Positions in list..- F\n    \n    style A fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style J fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    \n    style B fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style D fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style G fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style I fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n   \n    style C fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style E fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style F fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style H fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Text, Tokens, and Numbers</span>"
    ]
  },
  {
    "objectID": "module-01.html#the-ring-problem",
    "href": "module-01.html#the-ring-problem",
    "title": "1  Text, Tokens, and Numbers",
    "section": "1.3 The “Ring” Problem",
    "text": "1.3 The “Ring” Problem\nWe can now convert any text into a numerical matrix \\(X\\), where each token is represented by a row vector.\n\n1.3.1 What’s Missing?\nThere is no context in each token’s representation. Consider the word “ring”:\n\n“The ring of the telephone woke her up.”\n“He wore a golden ring on his finger.”\n\nIn the first sentence, “ring” is a sound. In the second, it’s jewelry. Yet in our current pipeline, both instances map to the exact same vector. The embedding matrix has only one row for “ring”: a static representation that captures the “average” meaning of the word across all its uses.\nThis is the core limitation: each token’s vector is computed in isolation. It encodes identity and position, but nothing about surrounding tokens (the context). Whether “telephone” or “golden” appears nearby, the representation of “ring” doesn’t change. All possible meanings remain collapsed into a single point—and a model that can’t resolve such ambiguities will fail at basic comprehension.\n\n\n1.3.2 What Comes Next\nThe next chapter introduces Self-Attention, the mechanism that finally incorporates context into each token’s representation.\nFor “ring,” this means:\n\nIn “The ring of the telephone,” the presence of “telephone” pulls the representation of “ring” toward its sound-meaning.\nIn “He wore a golden ring,” the presence of “golden” pulls the representation of “ring” toward its jewelry-meaning.\n\nThe static identity vectors we built here—the matrix \\(X\\)—will be transformed into dynamic, contextual representations. Identity becomes meaning. But that is a tale for another chapter.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Text, Tokens, and Numbers</span>"
    ]
  },
  {
    "objectID": "module-01.html#summary",
    "href": "module-01.html#summary",
    "title": "1  Text, Tokens, and Numbers",
    "section": "1.4 Summary",
    "text": "1.4 Summary\n\n1.4.1 The Pipeline So Far\n\nTokenization\n\nInput: Raw text (\"the ring fell\")\nOutput: Token IDs ([101, 205, 310])\n\nEmbedding Lookup\n\nInput: Token IDs\nOutput: Token embeddings \\(E\\) with shape \\((N \\times d_{model})\\)\n\nPositional Encoding\n\nInput: Position indices ([0, 1, 2])\nOutput: Position vectors \\(P\\) with shape \\((N \\times d_{model})\\)\n\nAddition\n\nInput: \\(E\\) and \\(P\\)\nOutput: Input matrix \\(X\\) with shape \\((N \\times d_{model})\\)\n\n\n\n\n1.4.2 Key Symbols\n\n\n\nSymbol\nName\nTypical Shape\n\n\n\n\n\\(N\\)\nSequence length\nScalar (e.g., 512, 2048)\n\n\n\\(d_{model}\\)\nModel dimension\nScalar (e.g., 768, 4096)\n\n\n\\(\\|V\\|\\)\nVocabulary size\nScalar (e.g., 50,000)\n\n\n\\(W_E\\)\nEmbedding matrix\n\\(\\|V\\| \\times d_{model}\\)\n\n\n\\(E\\)\nToken embeddings\n\\(N \\times d_{model}\\)\n\n\n\\(P\\)\nPositional encodings\n\\(N \\times d_{model}\\)\n\n\n\\(X\\)\nInput matrix\n\\(N \\times d_{model}\\)",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Text, Tokens, and Numbers</span>"
    ]
  },
  {
    "objectID": "module-02.html",
    "href": "module-02.html",
    "title": "2  The Fellowship of Tokens",
    "section": "",
    "text": "2.1 Meaning Arises from Context\n---\nconfig:\n  look: handDrawn\n  layout: \n  themeVariables:\n    fontFamily: \"Palatino Linotype, Book Antiqua, Garamond, Kristen ITC\"\n---\nflowchart LR\n    X(\"Ambiguous&lt;br&gt;Token\")\n    C(\"Context&lt;br&gt;(fellow tokens)\")\n    M(\"Meaning\")\n\n    X --&gt; M\n    C --&gt; M\n\n    style X fill:#e8dcc4,stroke:#000\n    style C fill:#d8e2dc,stroke:#000\n    style M fill:#fff,stroke:#000, stroke-width:2px\nIn Chapter 1, we learned to convert text into numbers—the input matrix X—so machines can process it. But remember the key problem? A word’s meaning shifts with context, yet we assign it the same embedding vector regardless of context.\nFor example, the word “ring” is represented by the same embedding vector whether it means a sound or a piece of jewelry.\nTherefore, the central problem for this chapter becomes: How can we make a token’s meaning change based on its context? To achieve this, we need to incorporate context. But what is context, exactly?\nWe can view context as the collective meaning of all tokens within a text. Under this view, the meaning of a token therefore relies on the meanings of its fellow tokens.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fellowship of Tokens</span>"
    ]
  },
  {
    "objectID": "module-02.html#mixing-tokens-by-relevance",
    "href": "module-02.html#mixing-tokens-by-relevance",
    "title": "2  The Fellowship of Tokens",
    "section": "2.2 Mixing Tokens by Relevance",
    "text": "2.2 Mixing Tokens by Relevance\nSo if a token’s context is just “other tokens,” can we simply average the other tokens? The answer is… no. Some words are more relevant to a particular word than others within the context.\nWe need to weigh some words more heavily than others. A token’s meaning should be something like: 20% my base meaning, 10% from this word, 70% from that word. But what determines these percentages?\n\n2.2.1 Why Relevance as the Weight?\nRelevance measures how strongly two tokens relate to each other within a given context. For example, in The ring of the telephone woke her up, word pairs have different levels of relevance:\n\nring and telephone: tightly connected (form a unit ring of the telephone)\nring and the: weakly connected (the is just a grammatical marker)\n\nWhy use relevance as the weight? Because strongly related tokens help shape each other’s contextual meaning. - If ring and telephone are tightly connected, then telephone carries important information about what ring means here (a sound, not a piece of jewelry). Weakly related tokens carry little such information.\n\nWith relevance-based weighting:\n\n\n\nToken\nRelevance to ring\n\n\n\n\ntelephone\n60%\n\n\nwoke\n15%\n\n\nthe\n10%\n\n\nher\n10%\n\n\nring (itself)\n5%\n\n\n\nNow tokens contribute according to how strongly they relate to ring, and therefore how much they tell us about ring’s meaning in this context.\nA token’s meaning is a mixture of tokens from a given context, weighted by relevance.\n\n\n\n\n2.2.2 The “Relevance Table”\nSince every token’s meaning depends on every other token, we need relevance weights for all possible token pairs. For a sequence of \\(N\\) tokens, this forms an \\(N \\times N\\) table where each row sums to 1:\nExample: ['the', 'ring', 'fell']\n\n\n\n\nthe\nring\nfell\n\n\n\n\nthe\n0.50\n0.30\n0.20\n\n\nring\n0.15\n0.25\n0.60\n\n\nfell\n0.20\n0.50\n0.30\n\n\n\nRead the “ring” row: ring and fell have the strongest relationship here (0.60), so fell contributes most to ring’s contextual meaning.\n\n\n\n\n\n\nNote\n\n\n\nRelevance doesn’t have to be mutual. ring can lean heavily on fell without fell leaning much on ring.\n\n\nThis table is precisely what the attention mechanism creates and what it uses to compute the contextual meaning of a token.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fellowship of Tokens</span>"
    ]
  },
  {
    "objectID": "module-02.html#attention-mechanism-the-intuition",
    "href": "module-02.html#attention-mechanism-the-intuition",
    "title": "2  The Fellowship of Tokens",
    "section": "2.3 Attention Mechanism: The Intuition",
    "text": "2.3 Attention Mechanism: The Intuition\n\n2.3.1 What is “attention”?\nBefore we talk about how the “attention mechanism” creates the “relevance table”, you might ask, what does “attention” mean here?\nWell, do not try to make sense of this word. We will come to that later.\nThe “attention mechanism” consists of two main stages:\n\nCompute the relevance between tokens.\nRetrieve and aggregate information by relevance.\n\n\n\n\n\n\n---\nconfig:\n  look: handDrawn\n  layout: elk\n  themeVariables:\n    fontFamily: \"Palatino Linotype, Book Antiqua, Garamond, Kristen ITC\"\n---\n%%{ init: { 'flowchart': { 'defaultRenderer': 'elk' } } }%%\nflowchart LR\n    X(\"Input Tokens\")\n    subgraph Attention[\"Attention Mechanism\"]\n        direction LR\n        A[\"**Stage 1**:&lt;br&gt;Compute Relevance\"]\n        B[\"**Stage 2**:&lt;br&gt;Retrieve Information by Relevance\"]\n        A --&gt; B\n    end\n    Z(\"Contextualized&lt;br&gt;Tokens\")\n\n    X --&gt; Attention --&gt; Z\n\n    style X fill:#d8e2dc,stroke:#000\n    style A fill:#e8dcc4,stroke:#000\n    style B fill:#fdf1b8,stroke:#000\n    style Z fill:#d8e2dc,stroke:#000\n    style Attention fill:#fff,stroke:#000\n\n\n\n\n\n\nAnd that’s all you need to know for now.\n\n\n2.3.2 More Specific Definition of Relevance\nIn the previous section, we defined relevance as “how strongly two tokens relate to each other within a given context”. To measure it, we need to be more specific.\nConsider two tokens, token \\(i\\) and token \\(j\\). We say token \\(j\\) is relevant to token \\(i\\) if token \\(j\\) influences the meaning of token \\(i\\), i.e. some sort of relevant information is passed from token \\(j\\) to token \\(i\\).\nAnd what is relevant information? - If something is relevant to me, it means that thing provides what I require in a given situation. - Likewise, the information that token \\(j\\) provides is relevant if that’s what token \\(i\\) requires in a given context. For example, an action token requires a token that acts as the subject in the context.\n(Let’s ignore what a token could possibly “require” for now.)\nIn other words, relevance is how well token \\(i\\)’s requirements align with what token \\(j\\) provides in a given context.\nIn transformer terminology, what a token requires is the “Query” vector (\\(q\\)) and what it can provide to others is the “Key” vector (\\(k\\)).\n\n\n\n\n\n\n\n\nTechnical Variable\nMetaphor\nIntuition\n\n\n\n\nQuery vector (\\(q\\))\nRequirements\nWhat information does this token require in this context?\n\n\nKey vector (\\(k\\))\nofferings\nWhat information can this token provide to others in this context?\n\n\n\n\n\n\n\n\n\nNoteWhat does a token “require”, anyway?\n\n\n\nWhy does a token ever “requires” anything? Well, because we tell it to. To be precise, during transformer training, our objective is to predict missing tokens that fit in well in a given text. (We’ll cover training later.) That means tokens have to sound natural, like what humans would say in real life (i.e. training data).\nThis requires a token’s meaning to align well with the meanings of other tokens in a given text, e.g. an action token “requires” a subject token in the same context. Computer scientists model this alignment using two numerical patterns (model weights), which represent every token’s relationship patterns in two directions.\nWe derive the Query and Key vectors from these relationship patterns stored in the form of model weights (see next section). In practice, Query and Key are just some numerical patterns, not human-readable categories like “subject” or “action”. These patterns can be interpreted as “requirements” and “offerings” or anything that help us understand what Query and Key do.\n\n\n\n\n2.3.3 Stage 1: Compute Relevance\nWe learn every token’s “requirements” and “offerings” (Query and Key vectors) through transformer training (see later sections). For a trained model, we already know what these values are.\nWhen we feed the tokens (in the form of \\(X\\)) into a trained model, it will derive the Query and Key vectors from our input (see Step 2 in the next section). It will then check for relevance between them.\nSince relevance involves both “requirements” and “offerings”, the simplest way is to multiply them together. We will need to do that for every dimension of Query and Key vectors (think of it as an aspect of “requirements” and the corresponding aspect of “offerings”), and then sum the numbers up. The most efficient way is to compute their dot product.\nFor tokens \\(i\\) and \\(j\\):\n\\[\n\\text{relevance}(i, j) = q_i \\cdot k_j\n\\]\nWe repeat this for every token-token combination to get the “relevance scores”. We can do this efficiently by matrix multiplication:\n\\[\n\\text{Relevance scores, }S = Q \\cdot K^T\n\\]\nwhere \\(Q\\) and \\(K\\) are the matrices where each row is the \\(q\\) and \\(k\\) vectors of all tokens respectively. \\(K^T\\) is the transpose of \\(K\\).\nWe then convert the “relevance scores” to the attention matrix (A) so that the relevance scores become percentages. Let’s just treat it as a normalization for now:\n\\[\n\\text{Attention matrix, } A = \\text{Normalize}(\\text{Relevance scores})\n\\]\n\n\n2.3.4 Stage 2: Retrieve Information by Relevance\nWe’ve figured out who should influence whom (via relevance). But we also need to know what information actually gets passed.\nOnce we have the attention matrix \\(A\\), we use it to get the contextualized value \\(z\\) of token \\(i\\), which is the sum of the values of all tokens (including \\(i\\) itself) weighted by \\(A\\):\n\\[\nz_i = \\sum_{j=1}^{N} A_{ij} \\cdot v_j\n\\]\nwhere \\(v\\) is the Value vector, i.e. information carried by a token.\n\n\n2.3.5 What Changed after Attention?\n\n\n\n\nDim 1\nDim 2\nDim 3\nDim 4\n\n\n\n\nOriginal \\(v_{\\text{ring}}\\)\n0\n0\n2\n2\n\n\nContextualized \\(z_{\\text{ring}}\\)\n1.72\n0.19\n0.62\n2.15\n\n\n\n\\(v_{\\text{ring}}\\)’s dimensions 1 and 4 (where “fell” was strong) increased dramatically—\\(v_{\\text{ring}}\\) now carries information from “fell.”",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fellowship of Tokens</span>"
    ]
  },
  {
    "objectID": "module-02.html#why-is-it-called-attention",
    "href": "module-02.html#why-is-it-called-attention",
    "title": "2  The Fellowship of Tokens",
    "section": "2.4 Why Is It Called “Attention”?",
    "text": "2.4 Why Is It Called “Attention”?\nThe name comes from an anthropomorphic analogy: each token “pays attention” to other tokens.\n\nHigh attention weight → “I’m focusing on you”\nLow attention weight → “I’m ignoring you”\n\n\n2.4.1 Why the Metaphor Misleads\n\n\n\n\n\n\n\nMisconception\nReality\n\n\n\n\n“The model decides what to focus on”\nThere’s no decision-making agent. It’s matrix multiplication.\n\n\n“Attention weights explain reasoning”\nAttention shows information flow, not necessarily “why” the model made a prediction.\n\n\n“High attention = importance”\nA token can be important with low attention, or unimportant with high attention.\n\n\n“Attention is like human attention”\nHuman attention involves consciousness and intention. Self-attention is weighted averaging.\n\n\n\nThe term “attention” is ubiquitous—use it when communicating with others. But when reasoning about what the model actually does, remember:\nAttention is a relevance-weighted sum. Nothing more, nothing less.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fellowship of Tokens</span>"
    ]
  },
  {
    "objectID": "module-02.html#summary",
    "href": "module-02.html#summary",
    "title": "2  The Fellowship of Tokens",
    "section": "2.5 Summary",
    "text": "2.5 Summary\n\n2.5.1 Key Symbols\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nShape\n\n\n\n\n\\(X\\)\nInput token embeddings\n\\(N \\times d_{model}\\)\n\n\n\\(A\\)\nAttention matrix\n\\(N \\times N\\)\n\n\n\\(q_i\\)\nQuery vector of token \\(i\\)\n\\(1 \\times d_{model}\\)\n\n\n\\(k_i\\)\nKey vector of token \\(i\\)\n\\(1 \\times d_{model}\\)\n\n\n\\(v_i\\)\nValue vector of token \\(i\\)\n\\(1 \\times d_{model}\\)\n\n\n\\(z_i\\)\nContextualized representation of token \\(i\\)\n\\(1 \\times d_{model}\\)\n\n\n\n\n\n2.5.2 What Comes Next\nCongratulations! You now understand the essence of Attention, the hardest climb in the transformer landscape. Here’s what remains to explore:\n\nDeriving Q, K, V from weight matrices: Where do Query, Key, and Value vectors actually come from?\nNormalizing relevance scores: How and why we convert raw scores into percentages\nMulti-head attention: Running multiple attention mechanisms in parallel, each focusing on different types of relationships\nResidual connections: Adding the input back to the output\nLayer normalization: Stabilizing the values flowing through the network\nFeed-forward layers: Additional processing after attention\nStacking layers: Repeating the whole block multiple times\n\nThe next chapter will be a complete walkthrough—no detail left in shadow.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fellowship of Tokens</span>"
    ]
  },
  {
    "objectID": "module-03.html",
    "href": "module-03.html",
    "title": "3  Into the Depths of Attention",
    "section": "",
    "text": "3.1 Attention Step by Step\nIn Chapter 2, we built the intuition behind attention. Now let’s walk through the actual computation, step by step, with real numbers.\nHere’s the overall flow of self-attention:\n---\nconfig:\n  layout: elk\n  themeVariables:\n    fontFamily: \"monospace\"\n---\nflowchart BT\n    X(\"Input Matrix (X)\")\n    QKV[[\"Compute Q , K , V\"]]\n    Scores[[\"Compute Relevance&lt;br&gt;(Q × Kᵀ)\"]]\n    Softmax[[\"Scale and Softmax\"]]\n    Mix[[\"'Mix' Information&lt;br&gt;(A × V)\"]]\n    Z(\"Contextual Representations (Z)\")\n    \n    X --&gt; QKV --&gt; Scores --&gt; Softmax --&gt; Mix --&gt; Z\n    \n    style QKV fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style Scores fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style Softmax fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style Mix fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style X fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style Z fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\nWe’ll work through a complete example with real numbers.\nSentence: \"the ring fell\"\nTokens: [\"the\", \"ring\", \"fell\"], so \\(N = 3\\)\nDimensions: \\(d_k = d_v = 4\\)",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Into the Depths of Attention</span>"
    ]
  },
  {
    "objectID": "module-03.html#attention-step-by-step",
    "href": "module-03.html#attention-step-by-step",
    "title": "3  Into the Depths of Attention",
    "section": "",
    "text": "3.1.1 Step 1: Start with Embeddings (\\(X\\))\nFrom Chapter 1, each token has an embedding. Stack them into a matrix:\n\\[\nX = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\\\ 0.2 & 0.4 & 0.6 & 0.8 \\end{bmatrix} \\begin{matrix} \\leftarrow \\text{``the\"} \\\\ \\leftarrow \\text{``ring\"} \\\\ \\leftarrow \\text{``fell\"} \\end{matrix}\n\\]\n\n\n3.1.2 Step 2: Compute \\(Q\\), \\(K\\), \\(V\\)\nWe didn’t explain how a trained transformer derive the \\(Q\\), \\(K\\), and \\(V\\) from the input, but it’s straightforward. They’re computed by multiplying \\(X\\) by the transformer’s learned weight matrices (Shape: \\(d_{model} \\times d_k\\)):\n\\[\nQ = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n\\]\nwhere:\n\n\\(W^Q\\) — describes “how tokens express requirements”\n\\(W^K\\) — describes “how tokens express offerings”\n\\(W^V\\) — describes “what information tokens carry”\n\nFor our example with \\(d_{model}=d_k=d_v=4\\), assume training has learned these weight matrices:\n\\[\nW^Q = \\begin{bmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}, \\quad\nW^K = \\begin{bmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}, \\quad\nW^V = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 2 & 2 \\\\ 3 & 0 & 0 & 3 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\n\\]\nThen compute \\(Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\\):\n\\[\nQ = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\end{bmatrix}, \\quad K = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\end{bmatrix}, \\quad V = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 2 & 2 \\\\ 3 & 0 & 0 & 3 \\end{bmatrix}\n\\]\n\nThe ring example: Did you notice that the Query for ring (row 2 of Q \\(=[0, 1, 1, 0]\\)) and the Key for fell (row 3 of K \\(=[0, 1, 1, 0]\\)) are identical—This means what fell offers is exactly what ring requires!\n\n\n\n\n\n\n\nNoteContext Specificity comes from \\(X\\)\n\n\n\n\n\nNote that these \\(W\\) matrices are not context-specific: they describe the general linguistic patterns learned from the training data. What makes \\(Q\\), \\(K\\), and \\(V\\) context-specific is the input \\(X\\) (all the input tokens, i.e. “the context”).\n\n\n\n\n\n\n\n\n\nNoteDoes \\(V\\) have to be context-specific?\n\n\n\n\n\nNo. Indeed, in the simplest implementation, the information that a token carries is simply the token’s embedding in \\(X\\), i.e. \\(V = X\\). In practice, we often learn a separate \\(W^V\\) to transform the information carried by each token, as this gives the model more flexibility.\n\n\n\n\n\n3.1.3 Step 3: Compute Raw Scores (\\(S = QK^\\top\\))\nTo quantify the relevance, we compute the dot product between every Query and every Key:\n\\[\nS = QK^\\top = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 1 & 1 \\end{bmatrix}\n\\]\n\nThe ring example: ring scores highest with fell (2nd row 3rd column of \\(S=2\\))—exactly what we wanted.\n\n\n\n3.1.4 Step 4: Scale by \\(\\sqrt{d_k}\\)\nWhen \\(d_k\\) is large, dot products can become very large numbers (summing more numbers). Large numbers going into softmax create extreme distributions (almost all weight on one token), which harms learning.\nSolution: Divide by \\(\\sqrt{d_k}\\) to keep scores in a reasonable range.\n\\[\n\\tilde{S} = \\frac{S}{\\sqrt{d_k}} = \\frac{S}{2} = \\begin{bmatrix} 0.5 & 0 & 0 \\\\ 0 & 0.5 & 1.0 \\\\ 0 & 0.5 & 0.5 \\end{bmatrix}\n\\]\n\n\n\n\n\n\nNoteWhy \\(\\sqrt{d_k}\\) Specifically?\n\n\n\n\n\nIf entries of Q and K are roughly standard normal (mean 0, variance 1), then the dot product of two \\(d_k\\)-dimensional vectors has variance approximately \\(d_k\\). Dividing by \\(\\sqrt{d_k}\\) brings the variance back to approximately 1.\n\n\n\n\n\n3.1.5 Step 5: Softmax (Convert to Probabilities)\nWe want each row to be a probability distribution where all values lie between 0 and 1, and each row sums to 1 (like percentages). Softmax does exactly this:\n\\[\nA_{ij} = \\frac{\\exp(\\tilde{S}_{ij})}{\\sum_{j'} \\exp(\\tilde{S}_{ij'})}\n\\]\n\\[\nA \\approx \\begin{bmatrix} 0.45 & 0.27 & 0.27 \\\\ \\mathbf{0.19} & \\mathbf{0.31} & \\mathbf{0.51} \\\\ 0.23 & 0.38 & 0.38 \\end{bmatrix}\n\\]\n\nThe ring example: “ring” attends 51% to “fell,” 31% to itself, and 19% to “the.”\n\n\n\n3.1.6 Step 6: “Mix” Information\nNow we mix the information—retrieving and aggregating information from \\(V\\), weighted by the attention matrix \\(A\\):\n\\[\nZ = A \\cdot V\n\\]\n\\[\n= \\begin{bmatrix}\n0.45 & 0.27 & 0.27 \\\\\n0.19 & 0.31 & 0.51 \\\\\n0.23 & 0.38 & 0.38\n\\end{bmatrix}  \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 2 & 2 \\\\\n3 & 0 & 0 & 3\n\\end{bmatrix} = \\begin{bmatrix}\n1.26 & 0.45 & 0.54 & 1.35 \\\\\n\\mathbf{1.72} & \\mathbf{0.19} & \\mathbf{0.62} & \\mathbf{2.15} \\\\\n1.37 & 0.23 & 0.76 & 1.90\n\\end{bmatrix}\n\\]\n\nThe ring example: Row 2 (bolded) is the contextualized representation of ring.\n\n\n\n3.1.7 What Changed after Attention?\n\n\n\n\nDim 1\nDim 2\nDim 3\nDim 4\n\n\n\n\nOriginal \\(v_{\\text{ring}}\\)\n0\n0\n2\n2\n\n\nContextualized \\(z_{\\text{ring}}\\)\n1.72\n0.19\n0.62\n2.15\n\n\n\n\\(v_{\\text{ring}}\\)’s dimensions 1 and 4 (where “fell” was strong) increased dramatically—\\(z_{\\text{ring}}\\) now carries information from “fell.”\n\n\n3.1.8 The One Formula to Bind Them All\nWe can write everything in one line:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n\\]\nwhere \\(Q = XW^Q\\), \\(K = XW^K\\), \\(V = XW^V\\).\nHere is a simplified computational graph showing how variables are connected.\n\n\n\n\n\n---\nconfig:\n  themeVariables:\n    fontFamily: \"monospace\"\n  flowchart:\n    nodeSpacing: 35\n    rankSpacing: 30\n    padding: 10\n---\nflowchart LR\n    X(\"X\")\n    \n    subgraph Weights\n        direction TB\n        WQ(\"W&lt;sup&gt;Q&lt;/sup&gt;\")\n        WK(\"W&lt;sup&gt;K&lt;/sup&gt;\")\n        WV(\"W&lt;sup&gt;V&lt;/sup&gt;\")\n    end\n\n    Q(Q)\n    K(K)\n    V(V)\n    A(A)\n    KT(\"Kᵀ\")\n    S(\"S\")\n    A(\"A\")\n    Z(\"Z\")\n\n    X --- WQ & WK & WV\n    WQ --&gt; Q\n    WK --&gt; K\n    WV --&gt; V\n\n    Q --&gt; S\n    K --&gt; KT --&gt; S\n    S ---&gt;|\"scale &&lt;br&gt;softmax\"| A\n    A --&gt; Z\n    V --&gt; Z\n\n    style X fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style Z fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style WQ fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style WK fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style WV fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style Q fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style K fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style KT fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style V fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style S fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style A fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Into the Depths of Attention</span>"
    ]
  },
  {
    "objectID": "module-03.html#the-encoder-block",
    "href": "module-03.html#the-encoder-block",
    "title": "3  Into the Depths of Attention",
    "section": "3.2 The Encoder Block",
    "text": "3.2 The Encoder Block\nSelf-attention is the core of the transformer, but it doesn’t work alone. An Encoder Block wraps attention with additional components that stabilize training and enrich representations.\n\n\n\n\n\n\n\nComponent\nPurpose\n\n\n\n\nMulti-Head Attention\nLets tokens look at each other and gather context\n\n\nAdd & Norm\nStabilizes training and preserves original information\n\n\nFeed-Forward Network\nProcesses the gathered context within each token\n\n\n\nLet’s build up to this step by step.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Into the Depths of Attention</span>"
    ]
  },
  {
    "objectID": "module-03.html#multi-head-attention",
    "href": "module-03.html#multi-head-attention",
    "title": "3  Into the Depths of Attention",
    "section": "3.3 Multi-Head Attention",
    "text": "3.3 Multi-Head Attention\nConsider the word ring in our running example: \"the ring fell\".\nThe token ring might need to attend to: - fell for semantic role (what happened to the ring?) - the for grammatical context (is this a definite reference?)\nA single attention head must average these competing needs. Multi-head attention runs multiple attention operations in parallel, letting each head specialize in a different type of relationship.\n\n3.3.1 Parameters\n\n\n\nSymbol\nName\nValue\n\n\n\n\n\\(d_{model}\\)\nModel dimension\n4\n\n\n\\(h\\)\nNumber of heads\n2\n\n\n\\(d_k\\)\nDimension per head\n\\(d_{model} / h = 2\\)\n\n\n\n\n\n3.3.2 Step 1: Compute Q, K, V\nWe multiply \\(X\\) by the transformer’s learned weight matrices to get full-size matrices (\\(3 \\times 4\\)):\n\\[\nQ = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n\\]\nFor our example, assume training has learned:\n\\[\nQ = \\begin{bmatrix}\n1 & 0 & 0 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & 0\n\\end{bmatrix}, \\quad\nK = \\begin{bmatrix}\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1\n\\end{bmatrix}, \\quad\nV = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n2 & 2 & 0 & 0 \\\\\n0 & 0 & 3 & 3\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nNoteTwo Equivalent Views\n\n\n\n\n\nView 1: Project, then split (common in code) - First compute full \\(Q = XW^Q_{full}\\) (shape \\(3 \\times 4\\)) - Then split into \\(Q_1, Q_2\\) along the feature dimension\nView 2: Split projections (more intuitive) - Learn separate \\(W^{Q_1}, W^{Q_2}\\) (each \\(4 \\times 2\\)) - Compute \\(Q_1 = XW^{Q_1}\\) and \\(Q_2 = XW^{Q_2}\\) directly\nBoth produce the same result. In this chapter, we show the full matrices first to emphasize that all heads share the same input context.\n\n\n\n\n\n3.3.3 Step 2: Split into Heads\nWe split each matrix along the feature dimension into \\(h = 2\\) heads:\nHead 1 (columns 1–2): \\[\nQ_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad\nK_1 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad\nV_1 = \\begin{bmatrix} 1 & 1 \\\\ 2 & 2 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nHead 2 (columns 3–4): \\[\nQ_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 1 & 0 \\end{bmatrix}, \\quad\nK_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad\nV_2 = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 3 & 3 \\end{bmatrix}\n\\]\n\n\n3.3.4 Step 3: Parallel Attention\nEach head performs the full attention computation independently:\nHead 1 (captures semantic role: ring ↔︎ fell): \\[Z_1 = \\text{Attention}(Q_1, K_1, V_1) = \\begin{bmatrix} 1.5 & 1.5 \\\\ 1.8 & 1.8 \\\\ \\mathbf{1.6} & \\mathbf{1.6} \\end{bmatrix}\\]\nHead 2 (captures grammatical context: ring ↔︎ the): \\[Z_2 = \\text{Attention}(Q_2, K_2, V_2) = \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.2 & 1.2 \\\\ \\mathbf{2.4} & \\mathbf{2.4} \\end{bmatrix}\\]\n\n\n3.3.5 Step 4: Concatenate\nWe glue the head outputs back together along the feature dimension:\n\\[\nZ_{ring} = \\begin{bmatrix}\n1.5 & 1.5 & 1.0 & 1.0 \\\\\n1.8 & 1.8 & 1.2 & 1.2 \\\\\n1.6 & 1.6 & 2.4 & 2.4\n\\end{bmatrix}\n\\]\nShape is restored to \\(3 \\times 4\\).\n\n\n3.3.6 Step 5: Synthesize Insights\nWe multiply by \\(W^O\\) (\\(4 \\times 4\\)) to mix information across heads:\n\\[Z_{final} = Z_{ring} \\times W^O\\]\n\n\n\n\n\n\nNoteWhy output projection?\n\n\n\n\n\nAfter concatenation, information is segregated (columns 1–2 from Head 1, columns 3–4 from Head 2). The output projection \\(W^O\\) lets the model combine insights—e.g., “Head 1 found the semantic patient, Head 2 found the definite article”—into a unified representation.\n\n\n\n\n\n\n\n\n---\nconfig:\n  themeVariables:\n    fontFamily: \"monospace\"\n---\nflowchart BT\n    X(\"X (N × d&lt;sub&gt;model&lt;/sub&gt;)\") --&gt; Proj[[\"Compute Q, K, V\"]]\n    Proj --&gt; Split[[\"Split into Heads\"]]\n    Split --&gt; H1[[\"Head 1:&lt;br&gt;Attention\"]]\n    Split --&gt; H2[[\"Head 2:&lt;br&gt;Attention\"]]\n    H1 --&gt; Concat[[\"Concatenate\"]]\n    H2 --&gt; Concat\n    Concat --&gt; WO[[\"Synthesize Insights&lt;br&gt;(Z&lt;sub&gt;final&lt;/sub&gt; = Z · W&lt;sup&gt;O&lt;/sup&gt;)\"]]\n    WO --&gt; Z(\"Z&lt;sub&gt;final&lt;/sub&gt; (N × d&lt;sub&gt;model&lt;/sub&gt;)\")\n\n    style X fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style Z fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style H1 fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style H2 fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style Proj fill:#f4f4f4,stroke:#333\n    style Split fill:#f4f4f4,stroke:#333\n    style Concat fill:#f4f4f4,stroke:#333\n    style WO fill:#f4f4f4,stroke:#333",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Into the Depths of Attention</span>"
    ]
  },
  {
    "objectID": "module-03.html#completing-the-block-residuals-norm-and-ffn",
    "href": "module-03.html#completing-the-block-residuals-norm-and-ffn",
    "title": "3  Into the Depths of Attention",
    "section": "3.4 Completing the Block: Residuals, Norm, and FFN",
    "text": "3.4 Completing the Block: Residuals, Norm, and FFN\nMulti-Head Attention produces a contextualized matrix \\(Z_{final}\\), but we need a few more components to form a complete Encoder Block.\n\n\n\n\n\n---\nconfig:\n  layout: elk\n  themeVariables:\n    fontFamily: \"monospace\"\n---\nflowchart BT\n    X(\"X&lt;sub&gt;Input&lt;/sub&gt; (N x d&lt;sub&gt;model&lt;/sub&gt;)\")\n    MHA[[\"Multi-Head Attention\"]]  \n    LN1[[\"Add & Norm\"]]\n    FFN[[\"Feed-Forward Network\"]]\n    LN2[[\"Add & Norm\"]]\n    Out(\"X&lt;sub&gt;Output&lt;/sub&gt; (N x d&lt;sub&gt;model&lt;/sub&gt;)\")\n\n    X --&gt; MHA --&gt; LN1 --&gt; FFN --&gt; LN2 --&gt; Out\n    X -- Copy --&gt; LN1\n    LN1 -- Copy --&gt; LN2\n\n    style X fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style Out fill:#d8e2dc,stroke:#5b7065,color:#2a3630,stroke-width:2px\n    style MHA fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style FFN fill:#fdf1b8,stroke:#9a832d,color:#4a3b2c\n    style LN1 fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n    style LN2 fill:#e8dcc4,stroke:#4a3b2c,color:#4a3b2c\n\n\n\n\n\n\n\n3.4.1 Residual Connection (Add)\nWe add the original input \\(X\\) to the attention output \\(Z_{final}\\):\n\\[X_{mid} = X + Z_{final}\\]\nFor our “ring” token, if row 2 of \\(Z_{final}\\) is \\([1.6, 1.6, 2.4, 2.4]\\) and row 2 of \\(X\\) is \\([0.5, 0.6, 0.7, 0.8]\\), then:\n\\[x_{\\text{ring,mid}} = [2.1, 2.2, 3.1, 3.2]\\]\nThis preserves the original token information while blending in context, creating a gradient highway for deep networks.\n\n\n3.4.2 Layer Normalization\nEach row (token) is normalized independently to have mean 0 and variance 1:\n\\[\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\\]\nFor \\(x_{\\text{ring,mid}} = [2.1, 2.2, 3.1, 3.2]\\):\n\n\\(\\mu = 2.65\\), \\(\\sigma \\approx 0.48\\)\nNormalized: \\([-1.15, -0.94, 0.94, 1.15]\\)\nScaled/shifted: \\(z_{\\text{ring}} = [1.2, 1.4, 2.6, 2.8]\\) (using learned \\(\\gamma, \\beta\\))\n\n\n\n3.4.3 Feed-Forward Network (FFN)\nEach token passes through a position-wise MLP. For our normalized “ring” token:\n\\[\\text{FFN}(z_{\\text{ring}}) = \\text{ReLU}(z_{\\text{ring}} W_1 + b_1)W_2 + b_2\\]\nExample with \\(d_{ff} = 4\\):\n\nInput: \\(z_{\\text{ring}} = [1.2, 1.4, 2.6, 2.8]\\) (contains motion from fell)\n\\(W_1\\) might learn: “If dimension 3 is high, activate a ‘motion’ neuron”\nReLU: \\(\\text{ReLU}([...])\\) keeps only positive activations\n\\(W_2\\) combines: Motion neuron + Original features → Richer representation\n\nResult: \\(z_{\\text{ring,final}} = [1.5, 1.3, 0.9, 2.9]\\)—now explicitly encoding “falling ring” semantics.\n\n\n\n\n\n\nNoteWhy FFN?\n\n\n\n\n\nAttention mixes context between tokens. The FFN processes this mixture within each token, enabling non-linear refinement that attention alone cannot perform.\n\n\n\n\n\n3.4.4 Another Residual + Norm\nWe repeat the pattern:\n\\[X_{output} = \\text{LayerNorm}(X_{mid} + \\text{FFN}(X_{mid}))\\]\nThe final output maintains the same \\(3 \\times 4\\) shape as the input, ready for the next Encoder Block. Stacking 12–96 such blocks builds progressively deeper understanding.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Into the Depths of Attention</span>"
    ]
  },
  {
    "objectID": "module-03.html#summary",
    "href": "module-03.html#summary",
    "title": "3  Into the Depths of Attention",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\n3.5.1 What Self-Attention Computes\n\nRelevance scores between every pair of tokens (via \\(Q \\cdot K\\) dot products)\nNormalized weights (via scaling and softmax) forming the attention matrix \\(A\\)\nNew representations (via weighted sum of \\(V\\)) that blend information from relevant tokens\n\n\n\n3.5.2 The Formula\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n\\]\n\n\n3.5.3 Key Symbols\n\n\n\nSymbol\nMeaning\nShape\n\n\n\n\n\\(N\\)\nSequence length\nScalar\n\n\n\\(d_{model}\\)\nModel dimension\nScalar\n\n\n\\(h\\)\nNumber of heads\nScalar\n\n\n\\(d_k\\)\nHead dimension\n\\(d_{model} / h\\)\n\n\n\\(X\\)\nInput token embeddings\n\\(N \\times d_{model}\\)\n\n\n\\(W^Q, W^K\\)\nQuery/Key projections\n\\(d_{model} \\times d_k\\)\n\n\n\\(W^V\\)\nValue projection\n\\(d_{model} \\times d_v\\)\n\n\n\\(W^O\\)\nAttention Output projection\n\\(d_{model} \\times d_{model}\\)\n\n\n\\(Q, K, V\\)\nQueries, Keys & Values\n\\(N \\times d_k\\)\n\n\n\\(A\\)\nAttention matrix\n\\(N \\times N\\)\n\n\n\\(Z\\)\nOutput matrix\n\\(N \\times d_v\\)\n\n\n\n\n\n3.5.4 What Comes Next\nWe’ve built the complete Encoder Block. But we haven’t talked about:\n\nHow the model learns the weight matrices \\(W^Q\\), \\(W^K\\), \\(W^V\\)\nPretraining objectives that teach the model language patterns\nDecoder blocks and how they differ from encoders\n\nThese are the stories for the next chapter.",
    "crumbs": [
      "The One Mechanism",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Into the Depths of Attention</span>"
    ]
  },
  {
    "objectID": "module-04.html",
    "href": "module-04.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Unfinished Tale",
    "crumbs": [
      "The Twofold Training (TBD)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>module-04.html</span>"
    ]
  },
  {
    "objectID": "module-05.html",
    "href": "module-05.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Unfinished Tale",
    "crumbs": [
      "The Twofold Training (TBD)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>module-05.html</span>"
    ]
  },
  {
    "objectID": "module-06.html",
    "href": "module-06.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Unfinished Tale",
    "crumbs": [
      "The Three Wizards (TBD)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>module-06.html</span>"
    ]
  },
  {
    "objectID": "module-07.html",
    "href": "module-07.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Unfinished Tale",
    "crumbs": [
      "The Three Wizards (TBD)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>module-07.html</span>"
    ]
  },
  {
    "objectID": "module-08.html",
    "href": "module-08.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Unfinished Tale",
    "crumbs": [
      "The Appendices (TBD)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>module-08.html</span>"
    ]
  },
  {
    "objectID": "module-09.html",
    "href": "module-09.html",
    "title": "A Plain Walkthrough of Transformer Models",
    "section": "",
    "text": "Unfinished Tale",
    "crumbs": [
      "The Appendices (TBD)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>module-09.html</span>"
    ]
  }
]